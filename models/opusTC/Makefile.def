# -*-makefile-*-



METRICS := bleu spbleu chrf chrf++ comet

## set the home directory of the repository
## this is to find the included makefiles
## (important to have a trailing '/')

SHELL    := bash
PWD      := ${shell pwd}
TODAY    := $(shell date +%F)
REPOHOME := ${PWD}/../../

include ${REPOHOME}lib/env.mk
include ${REPOHOME}lib/config.mk
include ${REPOHOME}lib/slurm.mk

GPUJOB_HPC_MEM = 20g




MODEL_STORAGE     := https://object.pouta.csc.fi/Tatoeba-MT-models
ifndef MODEL_DISTS
ifneq ($(wildcard models.missing),)
  MODEL_DISTS     := $(shell cat models.missing)
else
  MODEL_DISTS     := ${shell ${WGET} -q -O - ${MODEL_STORAGE}/index.txt | grep '.zip$$' | grep -v '.eval.zip$$'}
endif
endif

MODEL_DIST         = ${firstword ${MODEL_DISTS}}
MODEL              = ${MODEL_DIST:.zip=}
MODEL_LANGPAIR     = ${firstword ${subst /, ,${MODEL_DIST}}}
MODEL_URL          = ${MODEL_STORAGE}/${MODEL_DIST}
MODEL_EVAL_URL     = ${MODEL_URL:.zip=.eval.zip}


## directory with all test sets (submodule OPUS-MT-testsets)

TESTSET_HOME   := ${REPOHOME}OPUS-MT-testsets/testsets
TESTSET_INDEX  := ${REPOHOME}OPUS-MT-testsets/index.txt


## work directory (for the temporary models)

WORK_HOME      = ${PWD}/work
WORK_DIR       = ${WORK_HOME}/${MODEL}


## model directory (for test results)
## model score file and zipfile with evaluation results

# MODEL_HOME      = ${REPOHOME}tatoeba/models
MODEL_HOME      = ${PWD}
MODEL_DIR       = ${MODEL_HOME}/${MODEL}
MODEL_EVALZIP   = ${MODEL_DIR}.eval.zip
LEADERBOARD_DIR = ${REPOHOME}scores



## all zip files with benchmark results
MODEL_EVALZIPS := ${patsubst %.zip,${MODEL_HOME}/%.eval.zip,${MODEL_DISTS}}


## score files with all evaluation results
##   - combination of BLEU and chrF (MODEL_SCORES)
##   - for a specific metric (MODEL_METRIC_SCORES)
##   - all score files (MODEL_EVAL_SCORES)

MODEL_SCORES        = ${MODEL_DIR}.scores.txt
MODEL_METRIC_SCORES = $(patsubst %,${MODEL_DIR}.%-scores.txt,${METRICS})
MODEL_EVAL_SCORES   = ${MODEL_SCORES} ${MODEL_METRIC_SCORES}
